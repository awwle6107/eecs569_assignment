{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5367186",
   "metadata": {},
   "source": [
    "# EECS 469/569: Homework 3 Submission\n",
    "## Multi-Node Performance of Roaring Thunder\n",
    "### Due: Tuesday, Nov. 1 *before* midnight\n",
    "\n",
    "(Optional this time) [Sign-up for a grading period here.](https://doodle.com/meeting/participate/id/ejqXYGzd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8eef32",
   "metadata": {},
   "source": [
    "## Name 1:\n",
    "## Name 2: \n",
    "## Name 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd3cfb",
   "metadata": {},
   "source": [
    "## Checklist\n",
    "\n",
    "0. [Other Deliverables](#Other-Deliverables)\n",
    "    0. Sign up for grading slot (Optional, but encouraged)\n",
    "    1. Email team to Dr. Hansen\n",
    "    2. Write a couple of paragraphs on main takeaways and how you worked together\n",
    "1. [MPI Overhead](#1.-MPI-Overhead)\n",
    "    1. ping pong overhead (single node, two nodes)\n",
    "    2. collective communication overhead\n",
    "2. [MPI I/O](#2.-MPI-I/O)\n",
    "    1. MPI write speed\n",
    "    2. MPI read speed\n",
    "3. [MPI Linear Algebra](#3.-MPI-Linear-Algebra)\n",
    "    1. matrix-multiply speedup\n",
    "    2. matrix-vector and dot-product speedup\n",
    "4. [Other MPI Accelerations](#4.-Other-MPI-Accelerations)\n",
    "    1. description of your solved problem and how you solved it, discussion of another group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b32a39",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Follow all instructions within `hw03.ipynb`. To submit the homework assignment, put **only relevant files (including this notebook)** in a folder. Zip the folder (e.g., using [7-zip](https://www.7-zip.org/)) and send one email to Dr. Hansen (CC your partner) with the zipped folder. **Do not include the benchmark files, they are not relevant.** Print a .pdf of this (completed) Jupyter notebook and submit it to D2L before the deadline (`CTRL+P` $\\rightarrow$ `Save as PDF` in Google Chrome). \n",
    "\n",
    "It is your responsibility that all of the figures, plots, source code, etc. properly appear in the submitted notebook **and .pdf**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb25b6",
   "metadata": {},
   "source": [
    "## Other Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0cedb",
   "metadata": {},
   "source": [
    "$\\mathbf{\\infty}$**.1 FIRST DELIVERABLE (-5 points if not on time):** ***By class on Monday, Oct. 17,*** email Dr. Hansen (CC your group) who you will be working with for this homework. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062282d9",
   "metadata": {},
   "source": [
    "$\\mathbf{\\infty}$**.2 FINAL DELIVERABLE (3 points):** ***After*** you have completed the entire assignment, write a few paragraphs on your main takeaways from the assignment. **Clearly state** how the work was split up between each of your group members. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db79ae",
   "metadata": {},
   "source": [
    "## 1. MPI Overhead\n",
    "\n",
    "### Required SLURM Batch Submissions:\n",
    "* pingpong with 1 node 2 processes\n",
    "* pingpong with 2 nodes 1 process per node\n",
    "* collectives with 4 nodes and 32 processes per node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17956a26",
   "metadata": {},
   "source": [
    "**Datasets:** link your datasets here with a couple word description of each:\n",
    "* [example link](serial_sort.c)\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf53bc",
   "metadata": {},
   "source": [
    "**1.1 DELIVERABLE (8 points):** Collect the output of the pingpong into a .csv file (or other data format to be used for plotting). You will need the following: number of Bytes, MBytes/sec. Plot two lines on the same graph:\n",
    "1. pingpong on one node, MBytes/s (y-axis) vs. Bytes (x axis)\n",
    "2. pingpong on two nodes, MBytes/s vs. Bytes\n",
    "\n",
    "Be sure to add a legend and clearly identify which line is which. Discuss the difference in results for the pingpong benchmark for the two cases. What is different between the two cases, and why does that impact the transfer speed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of pingpong MBytes/s vs Bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46e5c5",
   "metadata": {},
   "source": [
    "**Discussion:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c967630",
   "metadata": {},
   "source": [
    "**1.2 DELIVERABLE (12 points):** Create three figures:\n",
    "1. two line plots for two benchmarks: overhead \\[t_avg\\] vs. number of processes\n",
    "2. one line plot for one benchmark: overhead \\[t_avg\\] vs. number of Bytes\n",
    "3. eight box-whiskers (one for each benchmark) comparing the overhead of each collective communication. \n",
    "    * you can approximate the true dataset by using a Normal distribution with mean \\[t_avg\\] and std. dev. $(t_{max}-t_{min})/4$.\n",
    "    \n",
    "For the collective communication operations, discuss how they compare in terms of communication overhead. Spend time discussing how each scale as a function of the number of processes, as well as the size of the message. Do you notice anything in particular as the messages go across nodes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae37615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overhead for two benchmarks here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the overhead for one benchmark here as a function of message size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b588cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the box-whiskers plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10989e",
   "metadata": {},
   "source": [
    "**discussion:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08addbe9",
   "metadata": {},
   "source": [
    "## 2. MPI I/O\n",
    "### Required SLURM Batch Submissions:\n",
    "* MPI Write (5 each): P = 2, 4, 8, 16, 32, 64\n",
    "* MPI Read (5 each): P = 2, 4, 8, 16, 32, 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04edc7b5",
   "metadata": {},
   "source": [
    "**Code and Datasets:** link your code and datasets here with a couple word description of each:\n",
    "* [example link](serial_sort.c)\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95dcc3",
   "metadata": {},
   "source": [
    "**2.1 DELIVERABLE:** Plot the average write speed (MB/s or other SI prefix) vs. P. Analyze and discuss your results (2-3 sentences). \n",
    "\n",
    "**Discussion:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average write speed vs. P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcade7b9",
   "metadata": {},
   "source": [
    "**2.2 DELIVERABLE:** Plot the average read speed (MB/s or other SI prefix) vs. P. Analyze and discuss your results (2-3 sentences). \n",
    "\n",
    "**Discussion:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894dcf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average read speed vs. P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d16aedf",
   "metadata": {},
   "source": [
    "**2.3 DELIVERABLE:** Create a box-whiskers plot that compares the read/write speed of the cluster for the $P$ that had the highest performance (make sure that each has the same $P$). Add two additional box-whiskers for the sequential read/write results that you obtained in HW1.  \n",
    "\n",
    "What are the key takeaways of parallel I/O vs. serial I/O? \n",
    "\n",
    "**Discussion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af39b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot box-whiskers here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6806c11",
   "metadata": {},
   "source": [
    "## 3. MPI Linear Algebra\n",
    "### Required SLURM Batch Submissions:\n",
    "* Matrix-Matrix Multiply, $N=4096$ (5 each): $P=4,16$\n",
    "    * (20 times) $P=64$\n",
    "* Matrix-Vector Product (5 each): $P = {1,2,4,8,16,32}$\n",
    "    * (20 times) $P=64$\n",
    "* Dot Product (5 each): $P = {1,2,4,8,16,32}$\n",
    "    * (20 times) $P=64$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61289ea2",
   "metadata": {},
   "source": [
    "**Code and Datasets:** link your code and datasets here with a couple word description of each:\n",
    "* [example link](serial_sort.c)\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03cc74",
   "metadata": {},
   "source": [
    "**3.1 DELIVERABLE:** Create three figures that have $P$ on the x-axis, and on the y-axis:\n",
    "* average parallel speedup (Use a $T=1$ time from Homework 1 or 2 from the serial version of the matmul code.)\n",
    "* average floating point operations per second (FLOPs)\n",
    "* average execution time\n",
    "\n",
    "***USE AN APPROPRIATE SI PREFIX FOR YOUR Y-AXES!*** \n",
    "\n",
    "**BONUS (5 points)**: extend your method to not need a square power-of-2 number of processes and also analyze $P=8,32,80$\n",
    "\n",
    "Discuss in a few sentences per figure the impact of MPI and the number of processes on algorithm performance. ***WHY*** do you think you are seeing the results you are? \n",
    "\n",
    "**Discussion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e72b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot average parallel speedup versus P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1091e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot average FLOPs versus P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot average execution time versus P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1999b7c1",
   "metadata": {},
   "source": [
    "**3.2 DELIVERABLE:** Create two figures that show the scaling of OpenMP versus MPI with $P$ (or $T$) on the x-axis, and on the y-axis:\n",
    "1. one plot each for OpenMP/MPI with parallel speedup (**include the ideal speedup as a third plot**)\n",
    "2. one plot each for OpenMP/MPI with FLOPs\n",
    "\n",
    "Discuss how the two parallel frameworks scale. \n",
    "\n",
    "**Discussion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a83a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot MPI and OpenMP: average parallel speedup versus P/T (include ideal speedup) WITH legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b02bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot MPI and OpenMP: average FLOPs versus P/T WITH legend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac782a",
   "metadata": {},
   "source": [
    "**3.3 DELIVERABLE:** \n",
    "\n",
    "Create two figures that have $P$ on the x-axis (including $T=1$), and on the y-axis:\n",
    "1. average parallel speedup versus the sequential time (plot the ideal speedup on the same graph)\n",
    "2. average floating point operations per second (FLOPs)\n",
    "\n",
    "Each figure should have three plots: one for the dot product, one for the matrix-vector product, and one for the matrix-matrix product. The two plots should have distinct colors and lines (e.g., solid verus dashed). Add a legend that clearly identifies which plot is which. \n",
    "\n",
    "Create a box-whiskers plot that shows the FLOPs performance compared between the matrix-matrix product, matrix-vector product, and dot-product for $P=64$. **Include both the MPI and OpenMP ($T=32$) results.** Discuss the results and make note of the performance between the OpenMP and MPI.\n",
    "\n",
    "**Discussion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5659f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average parallel speedup versus the sequential time (plot the ideal speedup on the same graph)\n",
    "# three plots: one for the dot product, one for the matrix-vector product, and one for the matrix-matrix product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efa369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average floating point operations per second (FLOPs) versus the sequential time (plot the ideal speedup on the same graph)\n",
    "# three plots: one for the dot product, one for the matrix-vector product, and one for the matrix-matrix product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#box-whiskers of FLOPs for P=64 (MPI) and T=32 (OpenMP)\n",
    "#6 plots: MPI matrix-matrix product, MPI matrix-vector product, MPI dot-product\n",
    "#         OpenMP matrix-matrix product, OpenMP matrix-vector product, OpenMP dot-product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3675d",
   "metadata": {},
   "source": [
    "## 4. Other MPI Accelerations\n",
    "### Required SLURM Batch Submissions:\n",
    "* solve your chosen problem (same as OpenMP) for $P=1,32,64$\n",
    "* choose one:\n",
    "    * parallel sort (5 each), $P=1,2,4,8,16,32,64$\n",
    "    * Cannon's algorithm: (5 each) $P=4,16$; (20 times) $P=64$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f199fbd9",
   "metadata": {},
   "source": [
    "**Code and Datasets:** link your datasets here with a couple word description of each:\n",
    "* [example link](serial_sort.c)\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44414f0b",
   "metadata": {},
   "source": [
    "**4.1 DELIVERABLE:** Describe what problem you chose and how you accelerated it using MPI; how does this differ from the OpenMP version? Prove that the parallel version is thread safe (same answer as non-threaded) and show the parallel speedup. \n",
    "\n",
    "Discuss your problem with another group and describe their problem in a few sentences, and which group (by name of team members). \n",
    "\n",
    "**Discussion of problem and MPI vs. OpenMP:**\n",
    "\n",
    "**Discussion with another group:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot or present any supporting evidence of your MPI acceleration (vs sequential and OpenMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90ae92",
   "metadata": {},
   "source": [
    "**4.2 DELIVERABLE** ***only perform the deliverable that goes with your chosen problem***\n",
    "\n",
    "#### Parallel Sort: \n",
    "1. Plot average parallel speedup vs. P (include the ideal speedup)\n",
    "2. Plot average execution time vs. P\n",
    "\n",
    "Discuss the scaling of this algorithm in a few sentences. Describe how you parallelized the sort. If you have time, you may also want to fix $P$ and vary $N$ to see how the algorithm scales as $\\mathcal{O}(N)$. Bonus points may be in order. \n",
    "\n",
    "**Discussion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average parallel speedup vs. P (include the ideal speedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec8a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average execution time vs. P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517cc3c1",
   "metadata": {},
   "source": [
    "**4.2 DELIVERABLE** ***only perform the deliverable that goes with your chosen problem***\n",
    "\n",
    "#### Cannon's Algorithm:\n",
    "Compare the performance of Cannon's to the other MPI implementation and OpenMP. Create two figures that have $P$ on the x-axis (including $T=1$), and on the y-axis:\n",
    "1. average parallel speedup versus the sequential time (plot the ideal speedup on the same graph)\n",
    "2. average floating point operations per second (FLOPs)\n",
    "\n",
    "Each figure should have three plots: one for OpenMP, one for original MPI, and one for Cannon's. Add a legend that clearly identifies which plot is which. \n",
    "\n",
    "Create a box-whiskers plot that shows the FLOPs performance compared between the three matrix multiply implementations. Discuss the results.\n",
    "\n",
    "**Discussion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade960b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average parallel speedup (plot the ideal speedup on the same graph)\n",
    "# three plots: one for OpenMP, one for original MPI, and one for Cannon's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f37282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average FLOPs (plot the ideal speedup on the same graph)\n",
    "# three plots: one for OpenMP, one for original MPI, and one for Cannon's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af95ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box-whiskers plot of FLOPs\n",
    "# three plots: OpenMP, MPI original, Cannon's"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
